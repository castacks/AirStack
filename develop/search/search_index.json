{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AirStack: Democratizing Intelligent Mobile Robotics","text":"<p>AirStack is a comprehensive, modular autonomy stack for embodied AI and robotics developed by the AirLab at Carnegie Mellon University's Robotics Institute. It provides a complete framework for developing, testing, and deploying autonomous mobile systems in both simulated and real-world environments.</p> <p> </p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Modular Architecture: Easily swap out components to customize for your specific needs</li> <li>ROS 2 Integration: Built on ROS 2 for robust inter-process communication</li> <li>Simulation Support: Integrated with NVIDIA Isaac Sim for high-fidelity simulation</li> <li>Multi-Robot Capability: Control and coordinate multiple robots simultaneously</li> <li>Ground Control Station: Monitor and control robots through an intuitive interface</li> <li>Comprehensive Autonomy Stack:</li> <li>Robot Interface Layer</li> <li>Sensor Integration</li> <li>Perception Systems</li> <li>Local Planning &amp; Control</li> <li>Global Planning</li> <li>Behavior Management</li> </ul>"},{"location":"#system-requirements","title":"\ud83d\udccb System Requirements","text":"<ul> <li>Docker: With NVIDIA Container Toolkit support</li> <li>NVIDIA GPU: RTX 3070 or better (for local Isaac Sim)</li> <li>Storage: At least 25GB free space for Docker images</li> <li>OS: Ubuntu 22.04 recommended</li> </ul>"},{"location":"#quick-start","title":"\ud83d\udd27 Quick Start","text":""},{"location":"#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone --recursive -j8 git@github.com:castacks/AirStack.git\ncd AirStack\n</code></pre>"},{"location":"#2-install-docker-with-nvidia-support","title":"2. Install Docker with NVIDIA Support","text":"<p>Follow NVIDIA's instructions for installing Docker with NVIDIA GPU support. Make sure <code>docker-compose-plugin</code> is also installed.</p>"},{"location":"#3-configure-the-repository","title":"3. Configure the Repository","text":"<pre><code>./configure.sh\n</code></pre> <p>Follow the prompts to complete the initial configuration.</p>"},{"location":"#4-get-the-docker-images","title":"4. Get the Docker Images","text":""},{"location":"#option-1-pull-from-airlab-registry-preferred","title":"Option 1: Pull from AirLab Registry (Preferred)","text":"<pre><code>docker login airlab-storage.andrew.cmu.edu:5001\n# Enter your andrew id (without @andrew.cmu.edu)\n# Enter your andrew password\n\n# Pull the images in the docker compose file\ndocker compose pull\n</code></pre>"},{"location":"#option-2-build-docker-images-from-scratch","title":"Option 2: Build Docker Images From Scratch","text":"<pre><code># Download the Ascent Spirit SITL software package\nbash simulation/isaac-sim/installation/download_sitl.bash\n\n# Build the images locally (requires NVIDIA NGC access)\ndocker compose build\n</code></pre>"},{"location":"#5-launch-the-system","title":"5. Launch the System","text":"<pre><code>xhost +  # allow docker access to X-Server\n\n# Start docker compose services\ndocker compose up -d\n# For multiple robots: docker compose up -d --scale robot=3\n</code></pre> <p>This will automatically launch and play the Isaac Sim scene specified in the <code>.env</code> file.</p>"},{"location":"#6-control-the-robot","title":"6. Control the Robot","text":"<p>Find the RQT GUI window: 1. Click \"Arm and Takeoff\" 2. Click \"Global Plan\" in the trajectory window</p> <p>You can also switch to \"Fixed Trajectory\" mode and click \"Publish\" to follow a predefined trajectory.</p>"},{"location":"#7-shutdown","title":"7. Shutdown","text":"<pre><code>docker compose down\n</code></pre>"},{"location":"#system-architecture","title":"\ud83c\udfd7\ufe0f System Architecture","text":"<p>AirStack follows a layered architecture approach:</p> <pre><code>Robot\n\u251c\u2500\u2500 Interface Layer: Communication with robot controllers\n\u251c\u2500\u2500 Sensors Layer: Data acquisition from various sensors\n\u251c\u2500\u2500 Perception Layer: State estimation and environment understanding\n\u251c\u2500\u2500 Local Layer: \n\u2502   \u251c\u2500\u2500 World Model: Local environment representation\n\u2502   \u251c\u2500\u2500 Planning: Trajectory generation and obstacle avoidance\n\u2502   \u2514\u2500\u2500 Controls: Trajectory following\n\u251c\u2500\u2500 Global Layer:\n\u2502   \u251c\u2500\u2500 World Model: Global environment mapping\n\u2502   \u2514\u2500\u2500 Planning: Mission-level path planning\n\u2514\u2500\u2500 Behavior Layer: High-level decision making\n</code></pre>"},{"location":"#repository-structure","title":"\ud83d\udcc1 Repository Structure","text":"<ul> <li><code>robot/</code>: Contains the ROS 2 workspace for the robot autonomy stack</li> <li><code>ground_control_station/</code>: Software for monitoring and controlling robots</li> <li><code>simulation/</code>: Integration with Isaac Sim and simulation environments</li> <li><code>docs/</code>: Comprehensive documentation</li> <li><code>common/</code>: Shared libraries and utilities</li> <li><code>tests/</code>: Testing infrastructure</li> </ul>"},{"location":"#development","title":"\ud83e\uddea Development","text":"<p>AirStack is designed with modularity in mind, making it straightforward to extend or replace components. The development workflow is centered around Docker containers for consistent environments.</p> <p>For detailed development guidelines, see the Developer Guide.</p>"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Comprehensive documentation is available at https://docs.theairlab.org/docs/</p> <p>The documentation covers: - Getting started guides - Development workflows - Component descriptions - API references - Simulation setup - Real-world deployment</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions to AirStack! Please see our Contributing Guidelines for more information.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>AirStack is licensed under the Apache 2.0 or MIT license (to be finalized).</p>"},{"location":"#contact","title":"\ud83d\udce7 Contact","text":"<p>For questions or support, please contact the AirLab team at theairlab.org.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#100-2024-12-19","title":"[1.0.0] - 2024-12-19","text":"<p>First official public release.</p>"},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Docker image robot-l4t for Jetson AGX</li> <li>Automatically load and play Isaac Sim scene upon launch</li> <li>Random walk planner</li> <li>DROAN trajectory-library based local planner</li> <li>Initial GCS rviz capable of visualizing multiple robots</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>A bunch of stuff honestly</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Upgrade Isaac Sim from 4.1.0 to 4.2.0</li> <li>Unified docker image naming to use AirStack's version.</li> <li>Condensed GCS TAK docker images to single docker image</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Duplicate TAK images</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at {{ email }}. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"common/","title":"Index","text":"<p>Common top-level files for sub workspaces.</p> <p><code>ros_packages/</code> contains common ROS packages that are used between different machines, such as ground control station and the robot. This folder is mounted under the docker container under <code>~/ros_ws/src/common</code></p>"},{"location":"docs/about/","title":"About","text":"<p>This stack is built and maintained by the AirLab at Carnegie Mellon University's Robotics Institute. </p>"},{"location":"docs/about/#license","title":"License","text":"<p>Not sure yet but probably Apache 2.0 or MIT for the open source parts.</p>"},{"location":"docs/about/#faq","title":"FAQ","text":"<p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"docs/getting_started/","title":"Getting Started","text":"<p>By the end of this tutorial, you will have the autonomy stack running on your machine.</p>"},{"location":"docs/getting_started/#requirements","title":"Requirements","text":"<p>You need at least 25GB free to install the Docker image.</p> <p>Have an NVIDIA GPU &gt;= RTX 3070 to run Isaac Sim locally.</p>"},{"location":"docs/getting_started/#setup","title":"Setup","text":""},{"location":"docs/getting_started/#clone","title":"Clone","text":"<pre><code>git clone --recursive -j8 git@github.com:castacks/AirStack.git\n</code></pre>"},{"location":"docs/getting_started/#docker","title":"Docker","text":"<p>Follow NVIDIA's instructions for installing Docker to be compatible with NVIDIA GPUs, including adding the NVIDIA Container Toolkit. Make sure <code>docker-compose-plugin</code> is also installed with Docker.</p>"},{"location":"docs/getting_started/#configure","title":"Configure","text":"<p>Run <code>./configure.sh</code> and follow the instructions in the prompts to do an initial configuration of the repo.</p>"},{"location":"docs/getting_started/#docker-images","title":"Docker Images","text":"<p>Now you have two options on how to proceed. You can build the docker image from scratch or pull the existing image on the airlab docker registry. Building the image from scratch can be useful if you would like to add new dependencies or add new custom functionality. For most users just pulling the existing image will be more conveninent and fast since it doesn't require access to the Nvidia registry.</p> Option 1: Pull From the Airlab Registry (Preferred) To use the AirLab Docker registry do the following  <pre><code>cd AirStack/\ndocker login airlab-storage.andrew.cmu.edu:5001\n## &lt;Enter your andrew id (without @andrew.cmu.edu)&gt;\n## &lt;Enter your andrew password&gt;\n\n## Pull the images in the docker compose file\ndocker compose pull\n</code></pre>  The images will be pulled from the server automatically. This might take a while since the images are large.   Option 2: Build Docker Images From Scratch  1.  Download the Ascent Spirit SITL software package by running this script (pip3 is required):      <pre><code>cd AirStack/\nbash simulation/isaac-sim/installation/download_sitl.bash\n</code></pre>  2.  Next, gain access to NVIDIA NGC Containers by following these instructions.      Then:      <pre><code>cd AirStack/\ndocker compose build  # build the images locally\n</code></pre>  If you have permission you can push updated images to the docker server.  <pre><code>docker compose push\n</code></pre>"},{"location":"docs/getting_started/#launch","title":"Launch","text":"<pre><code>./launch.sh # This will launch the docker containers, isaac sim, and WinTAK\n</code></pre> <p>This will automatically launch and play the Isaac scene specified under <code>AirStack/.env</code> (default is the Fire Academy).</p>"},{"location":"docs/getting_started/#move-robot","title":"Move Robot","text":"<p>Find the RQT GUI window. Hit <code>Arm and Takeoff</code>, then hit <code>Global Plan</code> in the trajectory window like in this video:</p> <p>You can also switch to <code>Fixed Trajectory</code> mode and hit <code>Publish</code> on the bottom right to fly a predefined trajectory.</p>"},{"location":"docs/getting_started/#shutdown","title":"Shutdown","text":"<p>To shutdown and remove docker containers:</p> <pre><code>./shutdown.sh # This will stop and remove the docker containers, isaac sim, and WinTAK\n</code></pre>"},{"location":"docs/development/","title":"Developer Guide","text":"<p>Welcome developers! This guide documents how to extend the autonomy stack for your own needs. The stack has been designed with modularity in mind, and aims to make it straight forward to swap out any component.</p> <p>We assume you're developing first on a local machine with simulation.</p>"},{"location":"docs/development/contributing/","title":"Contributing","text":"<p>This page describes how to merge content back into main.</p>"},{"location":"docs/development/contributing/#dependencies","title":"Dependencies","text":"<p>Make sure to add your ROS2 package dependencies to your <code>package.xml</code> file. These get installed when the docker image is built.</p> <p>If you need to add a dependency that's not in the docker image, please add a section to the <code>Dockerfile</code> in the <code>docker/</code> directory.</p>"},{"location":"docs/development/contributing/#documentation","title":"Documentation","text":"<p>Please make sure to document your work. Docs are under <code>AirStack/docs/</code>. The navigation tree is under <code>AirStack/mkdocs.yml</code>.</p> <p>This documentation is built with Material MKDocs. Visit mkdocs.org and mkdocs-material to learn how to use it.</p>"},{"location":"docs/development/contributing/#commands","title":"Commands","text":"<p><pre><code>pip install mkdocs-material\nmkdocs serve\n</code></pre> Launches docs on https://localhost:8000.</p> <ul> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"docs/development/contributing/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"docs/development/contributing/#merge","title":"Merge","text":"<p>Submit a pull request.</p> <p>All tests must pass before merging.</p> <p>Regression tests are run so that we don't break anything.</p>"},{"location":"docs/development/create_new_project/","title":"Create a New Project","text":""},{"location":"docs/development/create_new_project/#option-1-generate-from-template-no-further-updates-from-upstream","title":"Option 1: Generate from Template (no further updates from upstream)","text":"<p>The AirStack repository is setup as a Template on GitHub. This makes it easy to create a new project from AirStack with the \"Use this template\" button.</p> <p></p> <p>However, generating from a template squashes the entire git history into a single starter commit in your new repository.  This prevents pulling in updates from the AirStack repository in the future.</p> <p>To be able to pull upstream changes from the AirStack repository, use option 2.</p>"},{"location":"docs/development/create_new_project/#option-2-duplicate-for-future-updates-from-upstream","title":"Option 2: Duplicate (for future updates from upstream)","text":"<p>Duplicating a repository preserves the entire history of the repository, making it easier to pull in updates from AirStack in the future. Unlike creating a fork, duplicating a repository allows your new repository to be private.</p> <p>See GitHub's instructions to duplicate a repository.</p> <p>Sample commands are provided below: <pre><code>git clone --bare https://github.com/castacks/AirStack.git my-airstack\ncd my-airstack\ngit push --mirror https://github.com/EXAMPLE-USER/my-airstack.git\n</code></pre></p>"},{"location":"docs/development/docker_usage/","title":"Workflow with Docker and Docker Compose","text":"<p>To mimic interacting with multiple real world robots, we use Docker Compose to manage Docker containers that isolate the simulation, each robot, and the ground control station.</p> <p>The details of the docker compose setup is in the project root's <code>docker-compose.yaml</code>.</p> <p>In essence, the compose file launches:</p> <ul> <li>Isaac Sim</li> <li>ground control station</li> <li>robots</li> </ul> <p>all get created on the same default Docker bridge network. This lets them communicate with ROS2 on the same network.</p> <p>Each robot has its own ROS_DOMAIN_ID.</p>"},{"location":"docs/development/docker_usage/#pull-images","title":"Pull Images","text":"<p>To use the AirLab docker registry:</p> <pre><code>cd AirStack/\ndocker login airlab-storage.andrew.cmu.edu:5001\n## &lt;Enter your andrew id (without @andrew.cmu.edu)&gt;\n## &lt;Enter your andrew password&gt;\n\n## Pull the images in the docker compose file\ndocker compose pull\n</code></pre> <p>The available image tags are listed here.</p>"},{"location":"docs/development/docker_usage/#build-images-from-scratch","title":"Build Images From Scratch","text":"<pre><code>docker compose build\n</code></pre>"},{"location":"docs/development/docker_usage/#start-stop-and-remove","title":"Start, Stop, and Remove","text":"<p>Start</p> <pre><code>docker compose up --scale robot=[NUM_ROBOTS] -d\n\n# see running containers\ndocker ps -a\n</code></pre> <p>Stop</p> <pre><code>docker compose stop\n</code></pre> <p>Remove</p> <pre><code>docker compose down\n</code></pre> <p>Launch only specific services:</p> <pre><code># only robot\ndocker compose up robot --scale robot=[NUM_ROBOTS] -d\n# only isaac\ndocker compose up isaac-sim -d\n# only ground control station\ndocker compose up gcs -d\n</code></pre>"},{"location":"docs/development/docker_usage/#isaac-sim","title":"Isaac Sim","text":"<p>Start a bash shell in the Isaac Sim container:</p> <pre><code># if the isaac container is already running, execute a bash shell in it\ndocker exec -it isaac-sim bash\n# or if not, start a new container\ndocker compose run isaac-sim bash\n</code></pre> <p>Within the isaac-sim Docker container, the alias <code>runapp</code> launches Isaac Sim. The <code>--path</code> argument can be passed with a path to a <code>.usd</code> file to load a scene.</p> <p>It can also be run in headless mode with <code>./runheadless.native.sh</code> to stream to Omniverse Streaming Client or <code>./runheadless.webrtc.sh</code> to stream to a web browser.</p> <p>The container also has the isaacsim ROS2 package within that can be launched with <code>ros2 launch isaacsim run_isaacsim.launch.py</code>.</p>"},{"location":"docs/development/docker_usage/#robot","title":"Robot","text":"<p>Start a bash shell in a robot container, e.g. for robot_1:</p> <pre><code>docker exec -it airstack-robot-1 bash\n</code></pre> <p>The previous <code>docker compose up</code> launches robot_bringup in a tmux session. To attach to the session within the docker container, e.g. to inspect output, run <code>tmux attach</code>.</p> <p>The following commands are available within the robot container:</p> <pre><code># in robot docker\ncws  # cleans workspace\nbws  # builds workspace\nbws --packages-select [your_packages] # builds only desired packages\nsws  # sources workspace\nros2 launch robot_bringup robot.launch.xml  # top-level launch\n</code></pre> <p>These aliases are in <code>AirStack/robot/.bashrc</code>.</p> <p>Each robot has <code>ROS_DOMAIN_ID</code> set to its ID number. <code>ROBOT_NAME</code> is set to <code>robot_$ROS_DOMAIN_ID</code>.</p>"},{"location":"docs/development/docker_usage/#ground-control-station","title":"Ground Control Station","text":"<p>Currently the ground control station uses the same image as the robot container. This may change in the future.</p> <p>Start a bash shell in a robot container:</p> <pre><code>docker exec -it ground-control-station bash\n</code></pre> <p>The available aliases within the container are currently the same.</p> <p>On the GCS <code>ROS_DOMAIN_ID</code> is set to 0.</p>"},{"location":"docs/development/docker_usage/#ssh-into-robots","title":"SSH into Robots","text":"<p>The containers mimic the robots' onboard computers on the same network. Therefore we intend to interface with the robots through ssh.</p> <p>The <code>ground-control-station</code> and <code>docker-robot-*</code> containers are setup with ssh daemon, so you can ssh into the containers using the IP address.</p> <p>You can get the IP address of each container by running the following command:</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [CONTAINER-NAME]\n</code></pre> <p>Then ssh in, for example:</p> <pre><code>ssh root@172.18.0.6\n</code></pre> <p>The ssh password is <code>airstack</code>.</p>"},{"location":"docs/development/docker_usage/#container-details","title":"Container Details","text":"<pre><code>graph TD\n    A(Isaac Sim) &lt;-- Sensors and Actuation --&gt; B\n    A &lt;-- Sensors and Actuation --&gt; C\n    B(Robot 1) &lt;-- Global Info --&gt; D(Ground Control Station)\n    C(Robot 2) &lt;-- Global Info --&gt; D\n\n    style A fill:#76B900,stroke:#333,stroke-width:2px\n    style B fill:#fbb,stroke:#333,stroke-width:2px\n    style C fill:#fbb,stroke:#333,stroke-width:2px\n    style D fill:#fbf,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"docs/development/frame_conventions/","title":"Frame conventions","text":"<p>AirStack uses the East-North-Up (ENU) coordinate system. The robot's <code>map</code> frame is expected to be in ENU. However, the <code>world</code> frame is free to be in any coordinate system, as long as a suitable transform is provided.</p> <p>Isaac Sim follows the Forward-Left-Up (FLU) coordinate system. This means that the X-axis points forward, the Y-axis points left, and the Z-axis points up. More info on Isaac Sim frames can be found here.</p> <p>Read Scene Setup for more information on how Isaac Sim's FLU is converted to ENU for AirStack.</p>"},{"location":"docs/development/project_configuration/","title":"Project Configuration","text":"<p>The project as a whole can be configured using the <code>.env</code> file under the project root. The variables in the <code>.env</code> file get propagated into the <code>docker-compose.yml</code> files.</p> <p>The top-level env file is reproduced below: <pre><code># This top-level .env file under AirStack/ defines variables that are propagated through docker-compose.yaml\nPROJECT_NAME=\"airstack\"\n# auto-generated from git commit hash\nDOCKER_IMAGE_TAG=\"501d90f\"\n# can replace with your docker hub username\nPROJECT_DOCKER_REGISTRY=\"airlab-storage.andrew.cmu.edu:5001/shared\"\nDEFAULT_ISAAC_SCENE=\"omniverse://airlab-storage.andrew.cmu.edu:8443/Projects/AirStack/AFCA/fire_academy_faro_with_sky.scene.usd\"\nPLAY_SIM_ON_START=\"true\"\n# the file under robot/docker/ that contains the robot's environment variables\nROBOT_ENV_FILE_NAME=\"robot.env\"\n</code></pre></p>"},{"location":"docs/development/testing/","title":"Testing","text":""},{"location":"docs/development/testing/ci_cd/","title":"CI/CD Pipeline","text":""},{"location":"docs/development/testing/integration_testing/","title":"Integration Testing","text":""},{"location":"docs/development/testing/testing_frameworks/","title":"Testing Frameworks","text":""},{"location":"docs/development/testing/testing_frameworks/#testing-categories","title":"Testing Categories","text":"<p>Testing is organized into four main categories:</p> <ol> <li> <p>Integration Testing (End-to-End Testing):    Validates the entire system within its operational environment, whether in simulation or on hardware.</p> </li> <li> <p>System Testing:    Tests interactions between system components, such as communication between ROS nodes.</p> </li> <li> <p>Node Testing:    Focuses on verifying the functionality of individual nodes, from initialization to execution.</p> </li> <li> <p>Unit Testing:    Tests specific functions or business logic to ensure the correctness of the smallest units of code.</p> </li> </ol>"},{"location":"docs/development/testing/testing_frameworks/#testing-utilities","title":"Testing Utilities","text":"<p>Since our autonomy system primarily relies on ROS 2, we use the <code>colcon test</code> framework to run tests. Most tests are written in Python using the <code>pytest</code> package, as demonstrated in the example below.</p>"},{"location":"docs/development/testing/testing_frameworks/#testing-structure","title":"Testing Structure","text":"<p>All tests should be included in a <code>tests/</code> folder in their respective heirarchy of the architecture. For example, integration testing should on the same level as the <code>robot/</code> and <code>simulation/</code> folders, where a node test should reside in the ros package directoy. <code>colcon test</code> will search through the workspace to find all testing packages, provided they are specified in the Cmake.txt or setup.py files.</p>"},{"location":"docs/development/testing/testing_frameworks/#example-testing-script","title":"Example Testing Script","text":"<p>Below is an example of a systems test that can give the general structure of a testing script.</p> <pre><code>import os\nimport sys\nimport time\nimport unittest\nimport uuid\n\nimport launch\nfrom launch.launch_service import LaunchService\nimport launch_ros\nimport launch_ros.actions\nimport launch_testing.actions\nfrom launch_testing.io_handler import ActiveIoHandler\nimport launch_testing_ros\n\nimport pytest\n\nimport rclpy\nfrom rclpy.node import Node\n\nimport std_msgs.msg\nfrom std_msgs.msg import String\nimport mavros_msgs.srv\n\nimport time\n\n@pytest.mark.rostest\n# this is the test descriptioon used to launch the full robot system with launch_robot_headless.yaml\ndef generate_test_description():\n    robot_launch_path = 'path/to/launch/file'\n\n    gui_arg = launch.actions.DeclareLaunchArgument('use_gui', default_value='false', description='Whether to launch the GUI')\n\n    robot_launch = launch.actions.IncludeLaunchDescription( launch.launch_description_sources.AnyLaunchDescriptionSource(robot_launch_path),\n                                                            launch_arguments={'use_gui': launch.substitutions.LaunchConfiguration('use_gui')}.items())\n    return (\n        launch.LaunchDescription([\n            gui_arg,\n            robot_launch,\n            launch_testing.actions.ReadyToTest(),\n        ]),\n        {}\n    )\n\nclass TestRobotSystem(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # Initialize the ROS context for the test node\n        rclpy.init()\n\n    @classmethod\n    def tearDownClass(cls):\n        # Shutdown the ROS context\n        rclpy.shutdown()\n\n    def setUp(self):\n        # Create a ROS node for tests\n        self.node = rclpy.create_node('robot_tester_node')\n        self.service_timeout = 2\n\n    def tearDown(self):\n        self.node.destroy_node()\n\n    def test_set_mode(self):\n        client = self.node.create_client(mavros_msgs.srv.SetMode, '/mavros/set_mode')\n        self.node.get_logger().info(\"Waiting for service to be available...\")\n        accum_time = 0\n        while not client.wait_for_service(timeout_sec=1.0):\n            print('service not available, waiting again...')\n            accum_time += 1\n            if accum_time &gt; self.service_timeout:\n                print('service not available, aborting test...')\n                self.assertTrue(False)\n        request = mavros_msgs.srv.SetMode.Request()\n        request.custom_mode = \"GUIDED\"\n        print(\"Sending request to set mode to GUIDED\") \n        future = client.call_async(request)\n        rclpy.spin_until_future_complete(self.node, future)\n        response = future.result()\n        self.assertTrue(response.mode_sent)\n</code></pre>"},{"location":"docs/development/vscode/","title":"VS Code: Docker Integration and Debugger Setup","text":"<p>Start containers <pre><code># optionally pass the --scale robot=N argument to start N robots\ndc compose up -d  # --scale robot=2\n</code></pre></p> <p>Open AirStack folder</p> <pre><code>cd AirStack\ncode .\n</code></pre> <p>Install the \"Dev Containers\" extension.</p> <p>Now click the \"Remote Explorer\" icon on the left side bar, hover over a robot container, and attach to the container.</p> <p></p> <p>Install recommended extensions within the image. This installs the <code>ROS</code>, <code>C++</code>, and <code>Python</code> extensions in the container. </p>"},{"location":"docs/development/vscode/#build-ros-workspace","title":"Build ROS Workspace","text":"<p>Hit <code>Ctrl-Shift-B</code> to build the project. This is a shortcut for <code>bws --cmake-args '-DCMAKE_BUILD_TYPE=Debug'</code>, which adds debug symbols to the build.</p> <p>Build tasks are defined in <code>.vscode/tasks.json</code>.</p>"},{"location":"docs/development/vscode/#launch","title":"Launch","text":"<p>Hit <code>F5</code> to launch <code>robot.launch.xml</code>, or click the \"Run and Debug\" button on the left side of the screen and click the green play button.</p> <p>Launch tasks are defined in <code>.vscode/launch.json</code>.</p> <p></p> <p>You can now set breakpoints, view variables, step-through code, and debug as usual in VSCode.</p> <p></p> <p>Warning about file permissions</p> <p>Folders and files created within the attached docker container will be owned by root. This can cause issues when trying to edit files from the host machine, especially when using git to switch branches. If you accidentally create files as root, you can change the owner to your user with the following command: <pre><code>sudo chown -R $USER:$USER .\n</code></pre></p>"},{"location":"docs/ground_control_station/","title":"Ground Control Station","text":"<p>The Ground Control Station (GCS) is for operators to monitor and control the robots.</p> <p>Requirements:  - 60GB Hard Disk Space - min 8GB RAM - min 4 CPU Cores - Ubuntu 22.04</p>"},{"location":"docs/ground_control_station/#setup","title":"Setup","text":"<p>WinTAK is setup as auto start on boot and connects to Airlabs TAK-Server. Its run on Windows 11 VirtualBox Virtual Machine.</p> <p></p> <p>Run the following command from the setup folder  <pre><code># Move to the directory:\ncd ground_control_station/setup\n# Execute the script\n./setup_ground_control_station.sh\n</code></pre></p> <p>NOTE: If it asks to reset the password, please reset to current password.</p> <p></p>"},{"location":"docs/ground_control_station/#know-more-about-tak-using-the-youtube-link-below","title":"Know more about TAK using the youtube link below:","text":""},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/","title":"ROS2 CASEVAC Agent","text":""},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#overview","title":"Overview","text":"<p>ROS2 CASEVAC Agent is a service that bridges ROS2 casualty information to TAK (Tactical Assault Kit) systems using MQTT as the transport layer. The agent subscribes to ROS topics containing casualty metadata and images, converts this information into CoT (Cursor on Target) format, and publishes it to an MQTT broker for TAK systems to consume.</p>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#features","title":"Features","text":"<ul> <li>Subscribes to ROS casualty metadata and image topics</li> <li>Converts casualty information into standard CoT format with ZMIST fields</li> <li>Z: Zap Number - Unique casualty identifier</li> <li>M: Mechanism of Injury</li> <li>I: Injuries Sustained</li> <li>S: Signs and Symptoms</li> <li>T: Treatments Rendered</li> <li>Tracks multiple casualties simultaneously</li> <li>Handles various injury types and severity levels</li> <li>Transmits data over MQTT to TAK systems</li> </ul>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#installation","title":"Installation","text":""},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#prerequisites","title":"Prerequisites","text":"<ul> <li>ROS2 (tested with Foxy/Humble)</li> <li>Python 3.8+</li> <li>paho-mqtt</li> <li>PyTAK</li> </ul>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#dependencies","title":"Dependencies","text":"<pre><code>pip install paho-mqtt pytak pyyaml\n</code></pre>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#configuration","title":"Configuration","text":"<p>Create a YAML configuration file with the following structure:</p> <pre><code>project:\n  name: your_project_name\n\nservices:\n  host: your_host_ip\n  mediator:\n    ros2casevac_agent:\n      topic_name: to_tak  # MQTT topic name for CoT messages\n      ros_casualty_meta_topic_name: '/casualty/meta'  # ROS topic for casualty metadata\n      ros_casualty_image_topic_name: '/casualty/image'  # ROS topic for casualty images\n\nmqtt:\n  host: mqtt_broker_ip\n  port: mqtt_broker_port\n  username: mqtt_username\n  password: mqtt_password\n</code></pre>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#usage","title":"Usage","text":"<p>Run the agent with a configuration file:</p> <pre><code>ros2 run ros2tak_tools ros2casevac_agent --config path/to/your/config.yaml\n</code></pre>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#message-types","title":"Message Types","text":"<p>The agent expects the following ROS message types: - <code>airstack_msgs/CasualtyMeta</code>: Contains casualty metadata including:   - GPS coordinates   - Trauma assessments (head, torso, extremities)   - Vital signs (heart rate, respiratory rate)   - Critical conditions (hemorrhage, respiratory distress)   - Alertness indicators (ocular, verbal, motor)</p>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#how-it-works","title":"How It Works","text":"<ol> <li>The agent subscribes to the ROS topic for casualty metadata</li> <li>When new data is received, it updates an internal casualty tracking object</li> <li>If GPS data is available, it generates a CoT event in XML format</li> <li>The CoT event is published to the configured MQTT topic</li> <li>TAK systems subscribed to the MQTT topic receive and display the casualty information</li> </ol>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#customization","title":"Customization","text":"<p>The code supports several enum types for different injury categories: - <code>TraumaType</code>: Different body regions (head, torso, extremities) - <code>TraumaSeverity</code>: Levels of trauma (normal, wound, amputation) - <code>OcularAlertness</code>: Eye response states - <code>AlertnessLevel</code>: Verbal and motor response states - <code>VitalType</code>: Types of vital signs - <code>ConditionType</code>: Critical conditions - <code>ConditionStatus</code>: Presence/absence of conditions</p>"},{"location":"docs/ground_control_station/casualty_assessment/casualty_assessment/#author","title":"Author","text":"<p>Aditya Rauniyar (rauniyar@cmu.edu)</p>"},{"location":"docs/ground_control_station/command_center/command_center/","title":"Aerolens.ai - Talk to your robots","text":"<p>This configuration file defines the parameters for the <code>chat2ros_agent</code>, which connects to an MQTT topic, filters messages, and routes queries through ROS topics. <pre><code>chat2ros_agent:\n  mqtt_subcribe_topic: aerolens-ai  # Topic name at MQTT for the subscriber service that sends COT messages subscribed from the TAK server.\n  ros_query_text_topic: '/query/text'  # ROS Topic name to publish the chat queries.\n  ros_query_response_topic: '/query/response'  # ROS Topic name to publish the chat responses.\n  filter_name: &lt;project&gt;-operator\n</code></pre></p>"},{"location":"docs/ground_control_station/command_center/command_center/#parameters","title":"Parameters","text":"<ul> <li> <p><code>mqtt_subscribe_topic</code> (<code>aerolens-ai</code>):   The MQTT topic where messages containing COT data are received.</p> </li> <li> <p><code>ros_query_text_topic</code> (<code>/query/text</code>):   The ROS topic where filtered queries are published.</p> </li> <li> <p><code>ros_query_response_topic</code> (<code>/query/response</code>):   The ROS topic where responses to the queries are published.</p> </li> <li> <p><code>filter_name</code> (<code>dsta-operator</code>):   A filter applied to incoming messages, selecting only those where <code>filter_messages.name</code> matches this value.</p> </li> </ul>"},{"location":"docs/ground_control_station/command_center/command_center/#workflow","title":"Workflow","text":"<ol> <li>The service subscribes to the MQTT topic (<code>aerolens-ai</code>).</li> <li>It filters messages based on <code>filter_name</code> (<code>&lt;project&gt;-operator</code>).</li> <li>The extracted query is published to <code>&lt;robot_name&gt;/query/text</code>.</li> <li>The response to the query is expected on <code>&lt;robot_name&gt;/query/response</code>.</li> </ol> <p>This enables seamless integration between an MQTT-based message broker and ROS for structured communication.</p>"},{"location":"docs/ground_control_station/command_center/command_center/#commands","title":"Commands:","text":"<p><code>help</code> - Display the help message.</p> <p></p> <p><code>robot &lt;robot_name&gt; find &lt;object&gt;</code> - Find an object using the robot.</p>"},{"location":"docs/ground_control_station/usage/user_interface/","title":"User interface","text":""},{"location":"docs/ground_control_station/usage/user_interface/#the-tak-architecture-looks-like-follows","title":"The TAK Architecture looks like follows:","text":""},{"location":"docs/ground_control_station/usage/user_interface/#note-please-check-out-the-config-file-at-configyaml-to-understand-further-on-how-things-are-setup","title":"Note: Please check out the config file at config.yaml to understand further on how things are setup.","text":""},{"location":"docs/ground_control_station/usage/user_interface/#learn-how-to-use-the-tak-features","title":"Learn how to use the TAK features:","text":""},{"location":"docs/ground_control_station/usage/user_interface/#1-sending-robot-query-from-the-tak-chat","title":"1. Sending Robot Query from the TAK Chat.","text":""},{"location":"docs/ground_control_station/usage/user_interface/#2-displaying-automated-casevac-icons-from-the-casualty-inspections","title":"2. Displaying automated Casevac icons from the casualty inspections:","text":""},{"location":"docs/ground_control_station/usage/user_interface/#debugging-tips","title":"Debugging tips:","text":"<p>launch just the ground-control-station container: <pre><code>docker compose --profile deploy up ground-control-station\n````\n\n## 1. Running docker in interactive mode:\n```bash\ndocker exec -it ground-control-station /bin/bash\n</code></pre></p>"},{"location":"docs/ground_control_station/usage/user_interface/#2-checking-if-the-messages-are-being-received-by-the-mqtt-broker","title":"2. Checking if the messages are being received by the MQTT broker:","text":"<pre><code>mosquitto_sub -h localhost -t to_tak -u airlab # Default topic for sending messages to TAK\nmosquitto_sub -h localhost -t healthcheck -u airlab # Default topic for sending healthcheck messages\n</code></pre>"},{"location":"docs/real_world/","title":"Real World Overview","text":"<p>Fly robots in da wild.</p> <p>That's wild.</p>"},{"location":"docs/real_world/HITL/","title":"Hardware-In-The-Loop Simulation","text":"<p>We configure a multi-machine HITL simulation, where a powerful desktop computer runs Isaac Simulator and rendering, and one/multiple jetson compute boards run robot-specific programs (planning, mapping, etc.).</p>"},{"location":"docs/real_world/HITL/#requirement","title":"Requirement","text":"<p>A desktop computer configured according to here. One/multiple ORIN AGX/NX configured according to here.</p>"},{"location":"docs/real_world/HITL/#communication","title":"Communication","text":"<p>All machines should connect to the same network. In our test, all machines are connected to the same router with ethernet cables. Ensure that all machines are able to <code>ping</code> others' IP addresses.</p>"},{"location":"docs/real_world/HITL/#run","title":"Run","text":"<p>On the desktop computer, under your Airstack folder, run <pre><code>docker compose up isaac-sim-hitl\n</code></pre> You should see the isaac simulator being launched. On the Jetson computer, run <pre><code>docker compose up robot_l4t\n</code></pre> Once the scene is played in the Isaac simulator, the rviz GUI on the Jetson should start displaying sensor data, which means the connection is successful. </p> <p>Screen record of desktop computer:</p> <p>Screen record of Jetson computer:</p>"},{"location":"docs/real_world/data_offloading/","title":"Data Offloading","text":"<p>We have a tool to automatically offload and sync your robot data to the AirLab internal storage server.</p>"},{"location":"docs/real_world/data_offloading/#setup-storage-tools-server-locally","title":"Setup Storage Tools Server Locally","text":""},{"location":"docs/real_world/data_offloading/#clone-and-install","title":"Clone and install","text":"<pre><code>git clone https://github.com/castacks/storage_tools_server\ncd storage_tools_server\npython -m venv venv\n. venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"docs/real_world/data_offloading/#configure","title":"Configure","text":"<p>Edit the <code>config/config.yaml</code> file to match your configuration.</p>"},{"location":"docs/real_world/data_offloading/#required-updates","title":"REQUIRED UPDATES","text":"<ul> <li><code>upload_dir</code> is the location for uploads.  This must be readable and writeable by the user running the Server.</li> <li><code>volume_root</code> sets the prefix for all entries in the <code>volume_map</code>.  This must be readable and writeable by the user running the Server.</li> <li><code>volume_map</code> is a mapping from project name to <code>volume_root/{path}</code>.  All projects must have a mapping.</li> </ul>"},{"location":"docs/real_world/data_offloading/#set-environment-and-run","title":"Set Environment and Run","text":"<ul> <li><code>CONFIG</code> is the full path to the <code>config.yaml</code> in use.  By default, the app will use <code>$PWD/config/config.yaml</code></li> <li><code>PORT</code> is the same port as define in the optional setup. The default is 8091.</li> </ul> <pre><code>export CONFIG=$PWD/config/config.yaml\nexport PORT=8091\n\ngunicorn -k gevent -w 1 -b \"0.0.0.0:${PORT}\" --timeout 120 \"server.app:app\"\n</code></pre> <p>Open a web browser to http://localhost:8091 (or the PORT you set). The default user is <code>admin</code> and the default password is <code>NodeNodeDevices</code>.</p>"},{"location":"docs/real_world/data_offloading/#create-an-api-key-for-your-robot","title":"Create an API Key for your robot","text":"<ul> <li>Log into the Server</li> <li>Go to Configure -&gt; Keys</li> <li>Enter a name for the device key in the \"Add a new key name\" field.</li> <li>Click \"Generate Key\"</li> </ul>"},{"location":"docs/real_world/data_offloading/#set-up-storage-tools-device-on-your-robot","title":"Set up Storage Tools Device on your Robot","text":""},{"location":"docs/real_world/data_offloading/#install-requirements","title":"Install Requirements","text":"<ul> <li>Docker Compose</li> </ul>"},{"location":"docs/real_world/data_offloading/#clone-device-repo","title":"Clone Device Repo","text":"<pre><code>cd /opt \ngit clone https://github.com/castacks/storage_tools_device\ncd stroage_tools_device\n</code></pre>"},{"location":"docs/real_world/data_offloading/#update-the-configyaml","title":"Update the config.yaml","text":"<p>Update <code>config/config.yaml</code> to match your environment.  Things you should update:</p> <ul> <li><code>API_KEY_TOKEN</code>.  The api key that your admin gave you, or the key that you set up in the Server Setup</li> <li><code>watch</code>.  The list of directories that have your robot's files.</li> </ul> <p>Update the <code>env.sh</code> to match your system.</p> <ul> <li><code>CONFIG_FILE</code>.  If you have multiple config files, make sure <code>CONFIG_FILE</code> points to the one you want to use.</li> <li><code>DATA_DIR</code>. This is the top level data directory that all of the <code>watch</code> dirs share.  For example, if you <code>watch</code> directories are <code>/mnt/data/processor_1</code> and <code>/mnt/data/processor_2</code>, set the <code>DATA_DIR</code> to <code>/mnt/data</code>.  </li> </ul>"},{"location":"docs/real_world/data_offloading/#build-and-run","title":"Build and Run","text":"<pre><code>cd /opt/storage_tools_device\n. env.sh\ndocker compose up --build\n</code></pre>"},{"location":"docs/real_world/installation/","title":"Installation on ORIN AGX/NX","text":"<p>We have tested installation and running robot container on Jetson ORIN AGX/NX and Ubuntu 22.04.</p>"},{"location":"docs/real_world/installation/#setup","title":"Setup","text":"<p>Ensure you have docker installed.</p>"},{"location":"docs/real_world/installation/#clone","title":"Clone","text":"<p><pre><code>git clone --recursive -j8 git@github.com:castacks/AirStack.git\n</code></pre> Checkout to the correct branch: <pre><code>git checkout jkeller/jetson_36.4\n</code></pre></p>"},{"location":"docs/real_world/installation/#configure","title":"Configure","text":"<p>Run <code>./configure.sh</code> and follow the instructions in the prompts to do an initial configuration of the repo.</p> <p>Pull the correct image: <pre><code>docker compose pull robot_l4t\n</code></pre></p>"},{"location":"docs/real_world/installation/#run","title":"Run","text":"<p><pre><code>docker compose up robot_l4t\n</code></pre> You should be able to see the rviz GUI being launched.</p>"},{"location":"docs/robot/","title":"Robot","text":""},{"location":"docs/robot/#directory-structure","title":"Directory Structure","text":"<p>Underneath <code>AirStack/robot</code>, there are these directories: - <code>docker/</code>: Contains files related to building and launching the robot Docker container. - <code>installation/</code>: Contains files related to installing the robot software on a physical robot (TODO). - <code>ros_ws/</code>: Contains the ROS 2 workspace for the robot.</p>"},{"location":"docs/robot/#launch-structure","title":"Launch Structure","text":"<p>Each high-level module under <code>ros_ws/</code> has a <code>[module]_bringup</code> package that contains the launch files for that module. The launch files are located in the <code>launch</code> directory of the <code>[module]_bringup</code> package. The launch files are named <code>*.launch.(xml/yaml/py)</code> and can be launched with <code>ros2 launch &lt;module_name&gt;_bringup &lt;module_name&gt;.launch.(xml/yaml/py)</code>.</p> <p>At a high level, the launch files are organized as follows:</p> <pre><code>- robot_bringup/: robot.launch.xml\n    - autonomy_bringup/: autonomy.launch.xml\n        - interface_bringup/: interface.launch.xml\n        - sensors_bringup/: sensors.launch.xml\n        - perception_bringup/: perception.launch.xml\n        - local_bringup/: local.launch.xml\n        - global_bringup/: global.launch.xml\n        - behavior_bringup/: behavior.launch.xml\n</code></pre>"},{"location":"docs/robot/#configuration","title":"Configuration","text":""},{"location":"docs/robot/#desktop-vs-jetson","title":"Desktop vs Jetson","text":"<p>If you look at the <code>robot/docker/docker-compose.yaml</code> file, you'll see it contains two services. <code>robot</code> is meant for x86-64 desktop development whereas <code>robot_l4t</code> is meant to run on NVIDIA Jetson devices. Both extend a base service in <code>robot/docker/robot-base-docker-compose.yaml</code>.</p>"},{"location":"docs/robot/#environment-variables","title":"Environment Variables","text":"<p>Environment variables are used to configure the robot Docker container. The top level <code>AirStack/.env</code> file points to a <code>ROBOT_ENV_FILE_NAME</code> (default: <code>robot.env</code>), that in turn is used to load environment variables for the robot Docker container. The file that <code>ROBOT_ENV_FILE_NAME</code> points to gets added into the container under <code>robot/docker/robot-base-docker-compose.yaml</code>.</p> <p>The environment variables can be used to trigger nodes to launch. For example, the <code>USE_MACVO</code> environmental variable is checked by <code>perception.launch.xml</code> to determine whether to launch the <code>macvo</code> node.</p> <p>The file <code>robot.env</code> is reproduced below: <pre><code># These become environment variables in the robot container\nUSE_MACVO=\"true\"\n</code></pre></p>"},{"location":"docs/robot/#common-topics","title":"Common Topics","text":"Topic Type Description <code>/$ROBOT_NAME/odometry</code> nav_msgs/Odometry Best estimate of robot odometry <code>/$ROBOT_NAME/global_plan</code> nav_msgs/Path Current target global trajectory for the robot to follow. See global planning for more details."},{"location":"docs/robot/#rough-system-diagram","title":"Rough System Diagram","text":""},{"location":"docs/robot/autonomy/","title":"Autonomy Modules","text":""},{"location":"docs/robot/autonomy/#modules","title":"Modules","text":"<ul> <li>0_interface</li> <li>1_sensors</li> <li>2_perception</li> <li>3_local</li> <li>4_global</li> <li>5_behavior</li> </ul>"},{"location":"docs/robot/autonomy/#system-diagram","title":"System Diagram","text":""},{"location":"docs/robot/autonomy/0_interface/","title":"Robot Interface","text":"<p>The interface defines the communication between the autonomy stack running on the onboard computer and the robot's control unit. For example, for drones it converts the control commands from the autonomy stack into MAVLink messages for the flight controller.</p> <p>TODO: This is not our diagram, must replace. </p> <p>The code is located under <code>AirStack/ros_ws/src/robot/autonomy/0_interface/</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/0_interface/interface_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch interface_bringup interface.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#robotinterface","title":"RobotInterface","text":"<p>Package <code>robot_interface</code> is a ROS2 node that interfaces with the robot's hardware. The <code>RobotInterface</code> gets robot state and forwards it to the autonomy stack, and also translates control commands from the autonomy stack into the command for the underlying hardware. Note the base class is unimplemented. Specific implementations should extend <code>class RobotInterface</code> in <code>robot_interface.hpp</code>, for example <code>class MAVROSInterface</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#state","title":"State","text":"<p>The <code>RobotInterface</code> class broadcasts the robot's pose as a TF2 transform. It also publishes the robot's odometry as a <code>nav_msgs/Odometry</code> message to <code>$(env ROBOT_NAME)/0_interface/robot_0_interface/odometry</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#commands","title":"Commands","text":"<p>The commands are variations of the two main command modes: Attitude control and Position control. These are reflected in MAVLink and supported by both PX4 and Ardupilot.</p> <p>The RobotInterface node subscribes to:</p> <ul> <li><code>/$(env ROBOT_NAME)/interface/cmd_attitude_thrust</code> of type <code>mav_msgs/AttitudeThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_rate_thrust</code> of type <code>mav_msgs/RateThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_roll_pitch_yawrate_thrust</code> of type <code>mav_msgs/RollPitchYawrateThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_torque_thrust</code> of type <code>mav_msgs/TorqueThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_velocity</code> of type <code>geometry_msgs/TwistStamped.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_position</code> of type <code>geometry_msgs/PoseStamped.msg</code></li> </ul> <p>All messages are in the robot's body frame, except <code>velocity</code> and <code>position</code> which use the frame specified by the message header.</p>"},{"location":"docs/robot/autonomy/0_interface/#mavrosinterface","title":"MAVROSInterface","text":"<p>The available implementation in AirStack is called <code>MAVROSInterface</code> implemented in <code>mavros_interface.cpp</code>. It simply forwards the control commands to the Ascent flight controller (based on Ardupilot) using MAVROS.</p>"},{"location":"docs/robot/autonomy/0_interface/#custom-robot-interface","title":"Custom Robot Interface","text":"<p>If you're using a different robot control unit with its own custom API, then you need to create an associated RobotInterface. Implementations should do the following:</p>"},{"location":"docs/robot/autonomy/0_interface/#broadcast-state","title":"Broadcast State","text":"<p>Implementations of <code>RobotInterface</code> should obtain the robot's pose and broadcast it as a TF2 transform.</p> <p>Should look something like:</p> <pre><code>// callback function triggered by some loop\nvoid your_callback_function(){\n    // ...\n    geometry_msgs::msg::TransformStamped t;\n    // populate the transform, e.g.:\n    t.header = // some header\n    t.transform.translation.x = // some value\n    t.transform.translation.y = // some value\n    t.transform.translation.z = // some value\n    t.transform.rotation = // some quaternion\n    // Send the transformation\n    this-&gt;tf_broadcaster_-&gt;sendTransform(t);\n    // ...\n}\n</code></pre> <p>TODO: our code doesn't currently do it like this, it instead uses an external odometry_conversion node.</p>"},{"location":"docs/robot/autonomy/0_interface/#override-command-handling","title":"Override Command Handling","text":"<p>Should override all <code>virtual</code> functions in <code>robot_interface.hpp</code>:</p> <ul> <li><code>cmd_attitude_thrust_callback</code></li> <li><code>cmd_rate_thrust_callback</code></li> <li><code>cmd_roll_pitch_yawrate_thrust_callback</code></li> <li><code>cmd_torque_thrust_callback</code></li> <li><code>cmd_velocity_callback</code></li> <li><code>cmd_position_callback</code></li> <li><code>request_control</code></li> <li><code>arm</code></li> <li><code>disarm</code></li> <li><code>is_armed</code></li> <li><code>has_control</code></li> </ul>"},{"location":"docs/robot/autonomy/1_sensors/","title":"Index","text":"<p>We'll fill this with different things like the ZED-X package, LiDAR, etc</p>"},{"location":"docs/robot/autonomy/1_sensors/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/sensors/sensors_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch sensors_bringup sensors.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/1_sensors/gimbal/","title":"Gimbal Extension","text":""},{"location":"docs/robot/autonomy/1_sensors/gimbal/#overview","title":"Overview","text":"<p>The Gimbal Extension provides an easy way to integrate a controllable gimbal into an existing drone model within the scene. This extension is designed to facilitate the attachment and operation of a camera-equipped gimbal, allowing for real-time adjustments to pitch and yaw angles via ROS 2 messages.</p>"},{"location":"docs/robot/autonomy/1_sensors/gimbal/#installation-and-activation","title":"Installation and Activation","text":"<p>To enable the Gimbal Extension, follow these steps:</p> <ol> <li>Open the Extensions window by navigating to: Window \u2192 Extensions</li> <li>Under the THIRD PARTIES section, go to the User tab.</li> <li>Locate the Gimbal Extension and turn it on.</li> <li>Once enabled, a new Gimbal Extension window should appear.</li> </ol>"},{"location":"docs/robot/autonomy/1_sensors/gimbal/#adding-a-gimbal-to-a-drone","title":"Adding a Gimbal to a Drone","text":"<p>To attach a gimbal to an existing UAV model:</p> <ol> <li>Copy the prim path of the UAV to which you want to add the gimbal.</li> <li>In the Gimbal Extension window, paste the copied path into the Robot Prim Path text box.</li> <li>Set the Robot Index based on the <code>DOMAIN_ID</code> of the drone.  </li> <li>The <code>DOMAIN_ID</code> should match the identifier used for the robot to ensure proper communication.</li> </ol> <p>For a step-by-step demonstration, refer to the video tutorial below:</p>"},{"location":"docs/robot/autonomy/1_sensors/gimbal/#gimbal-camera-image-topic","title":"Gimbal Camera Image Topic","text":"<p>Once the gimbal is successfully added, the camera image feed from the gimbal will be published on the following ROS 2 topic: <code>/robot_&lt;ID&gt;/gimbal/rgb</code>.</p>"},{"location":"docs/robot/autonomy/1_sensors/gimbal/#controlling-the-gimbal","title":"Controlling the Gimbal","text":"<p>The gimbal pitch and yaw angles can be controled by the ros2 messages <code>/robot_&lt;ID&gt;/gimbal/desired_gimbal_pitch</code> and <code>/robot_&lt;ID&gt;/gimbal/desired_gimbal_yaw</code> of type <code>std_msgs/msg/Float64</code>, respectively.</p>"},{"location":"docs/robot/autonomy/2_perception/","title":"Perception","text":"<p>These modules process raw sensor data into useful information for the robot. For example: for detecting obstacles, localizing the robot, and recognizing objects.</p> <p>Perception modules typically output topics in image space or point cloud space. This information then gets aggregated into global and local world models later in the pipeline.</p> <p>Common perception modules include:</p> <ul> <li>semantic segmentation</li> <li>VIO (Visual Inertial Odometry)</li> </ul>"},{"location":"docs/robot/autonomy/2_perception/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/perception/perception_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch perception_bringup perception.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/2_perception/state_estimation/","title":"State estimation","text":"<p>Hey Yuheng your stuff goes here:)</p>"},{"location":"docs/robot/autonomy/3_local/","title":"Local Packages","text":"<p>The local module includes packages that are specific to the local autonomy of the robot. This includes local mapping, planning, and control.</p>"},{"location":"docs/robot/autonomy/3_local/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/local/local_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch local_bringup local.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/3_local/controls/","title":"Controls","text":"<p>Controls dictate the actuation of the robot. They are responsible for taking in sensor data and producing control commands. </p> <p>The controller should publish control commands directly to topics defined by the Robot Interface.</p> <p>Currently the AirStack uses a custom controller called \"Trajectory Controller\".</p>"},{"location":"docs/robot/autonomy/3_local/planning/","title":"Local Planning","text":"<p>Part of the local planner is the Waypoint Manager.</p> <p>The Waypoint Manager subscribes to the global waypoints and the drone's current position and publishes the next waypoint to the local planner.</p> <p>We plan for this baseline to be DROAN.</p>"},{"location":"docs/robot/autonomy/3_local/world_model/","title":"Local World Model","text":""},{"location":"docs/robot/autonomy/4_global/","title":"Global Packages","text":"<p>The global packages include global world models and planners.</p>"},{"location":"docs/robot/autonomy/4_global/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/global/global_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch global_bringup global.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/4_global/planning/","title":"Planning","text":"<p>Global planners output a high level, coarse trajectory for the robot to follow. </p> <p>A trajectory is a spatial path plus a schedule.  This means each waypoint in the trajectory has a time associated with it, indicating when the robot should reach that waypoint. These timestamps are fed to the local planner and controller to determine velocity and acceleration.</p> <p>If a waypoint's header timestamp is empty, the local planner should assume there's no time constraint and follow the trajectory at its own pace.</p> <p>The global planner should make a trajectory that is collision-free according to the global map. However, avoiding fine obstacles is delegated to the local planner that operates at a faster rate.</p> <p>For the structure of the package, the global planner node should not include any logic to generate the path. This should be located in a seperate logic class and be seperated from ROS. This will allow more modularity in the future for testing and easy interface changes.</p> <p>We intend the global planners to be modular. AirStack implements a basic Random Walk planner as a baseline.  Feel free to implement your own through the following interfaces.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#ros-interfaces","title":"ROS Interfaces","text":"<p>Global planners are meant to be modules that can be swapped out easily.  They can be thought of as different high level behaviors for the robot to follow. Consider that multiple global planners may be run in parallel, for example by some ensemble planner node that chooses the best plan for the current situation.</p> <p>As such, the global planner should be implemented as a ROS2 node that accepts runtime mission parameters in a custom <code>PlanRequest.msg</code> and  publishes a plan to its local <code>~/global_plan</code> topic.</p> <p>The best global plan should then be forwarded or remapped to <code>/$(env ROBOT_NAME)/global_plan</code> for the local planner to follow.</p> <pre><code>sequenceDiagram\n  autonumber\n  Global Manager-&gt;&gt;Global Planner: ~/plan_request (your_planner/PlanRequest.msg)\n  loop Planning\n      Global Planner--&gt;&gt;Global Manager: heartbeat feedback\n  end\n  Global Planner-&gt;&gt;Global Manager: ~/global_plan (nav_msgs/Path.msg)\n  Global Manager-&gt;&gt;Local Planner: /$ROBOT_NAME/global_plan_reference (nav_msgs/Path.msg)\n  Local Planner-&gt;&gt;Global Manager: /$ROBOT_NAME/global_plan_eta (nav_msgs/Path.msg)</code></pre>"},{"location":"docs/robot/autonomy/4_global/planning/#subscribe-plan-request","title":"Subscribe: Plan Request","text":"<p>Your custom <code>PlanRequest.msg</code> defines the parameters that your global planner needs to generate a plan.  It will be sent on the <code>~/plan_request</code> topic.</p> <p>Some common parameters may be the following: <pre><code># PlanRequest.msg\nstd_msgs/Duration timeout  # maximum time to spend planning\ngeometry_msgs/Polygon bounds # boundary that the plan must stay within\n</code></pre></p>"},{"location":"docs/robot/autonomy/4_global/planning/#publish-global-plan","title":"Publish: Global Plan","text":"<p>The global planner must publish a message of type <code>nav_msgs/Path</code> to <code>~/global_plan</code>. The message defines high level waypoints to reach by a given time.</p> <p>The <code>nav_msgs/Path</code> message type contains a <code>header</code> field and <code>poses</code> field.</p> <ul> <li>The top level header of <code>nav_msgs/Path</code> message should contain the coordinate frame of the trajectory, and its timestamp should indicate when the trajectory was published.</li> <li>Within the <code>poses</code> field, each <code>geometry_msgs/PoseStamped</code>'s header should contain a timestamp that indicates when that waypoint should be reached</li> </ul> <pre><code>nav_msgs/Path.msg\n    - std_msgs/Header header\n        - time stamp: when the trajectory was generated\n        - frame_id: the coordinate frame of the trajectory\n    - geometry_msgs/PoseStamped[] poses: the trajectory\n        - geometry_msgs/PoseStamped pose\n            - std_msgs/Header header\n                - time stamp: when the waypoint should be reached\n                - string frame_id: the coordinate frame of the waypoint\n            - geometry_msgs/Pose pose: the position and orientation of the waypoint\n</code></pre>"},{"location":"docs/robot/autonomy/4_global/planning/#publish-heartbeat","title":"Publish: Heartbeat","text":"<p>For long-running global planners, it's recommended to publish a heartbeat message to <code>~/heartbeat</code>. This way the calling node can know that the global planner is still running and hasn't crashed.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#additional-subscribers","title":"Additional Subscribers","text":"<p>In general, the global planner needs to access components of the world model such as the map and drone state.</p> <p>The most common map is Occupancy Grids that is published by TODO node.</p> <p>The global planner can also access the robot's current state and expected state in the future. For example, if the global planner takes 20 seconds to plan a trajectory,  it can query where the robot expects to be in 20 seconds. This ROS2 service is available under TODO.</p> <p>The global planner can do whatever it wants internally with this information.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#example-planners","title":"Example Planners","text":""},{"location":"docs/robot/autonomy/4_global/planning/#random-walk-planner","title":"Random Walk planner","text":"<p>The random walk planner replans when the robot is getting close to the goal. The random walk planner is a trivial planner that generates a plan by randomly selecting a direction to move in. The random walk planner is useful for testing the robot's ability to follow a plan.</p>"},{"location":"docs/robot/autonomy/4_global/world_model/","title":"World Model","text":"<p>Global world models are responsible for maintaining a representation of the world that is used by the global planner to generate a plan. This representation is typically a map of the environment, but can also include other information such as the location of other robots, obstacles, and goals.</p> <p>The current placeholder world model is a voxelized map representation called VDB Mapping.</p>"},{"location":"docs/robot/autonomy/5_behavior/","title":"Behavior","text":"<p>The behavior module is responsible for the high-level decision making of the robot. This includes deciding what actions to take based on the current state of the robot and the world around it. The behavior module is responsible for coordinating the actions of the local and global modules to achieve the robot's goals.</p>"},{"location":"docs/robot/autonomy/5_behavior/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/behavior/behavior_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch behavior_bringup behavior.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_executive/","title":"Behavior Executive","text":"<p>The behavior executive reads which actions are active from the behavior tree and implements the behavior which these actions should perform and sets the status of the actions to SUCCESS, RUNNING, or FAILURE. It also sets the status of conditions as either SUCCESS or FAILURE.</p> <p>A typical way of implementing the behavior for an action is the following in the 20 Hz timer callback:</p> <pre><code>if(action-&gt;is_active()){\n  if(action-&gt;active_has_changed()){\n    // This is only true when the when the action transitions between active/inactive\n    // so this block of code will only run once whenever the action goes from being inactive to active.\n    // You might put a service call here and then call action-&gt;set_success() or action-&gt;set_failure()\n    // based on the result returned by the service call.\n  }\n\n  // Code here will get executed each iteration.\n  // You might call action-&gt;set_running() while you are doing work here.\n}\n</code></pre>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/","title":"Behavior Trees","text":"<p>De\ufb01nes how a task in terms of conditions and actions which the user implements.</p> <p>Other types of nodes, control \ufb02ow and decorator nodes, control which conditions will be checked and which actions will be activated.</p> <p>Nodes have statuses of either SUCCESS, RUNNING or FAILURE.</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#why-behavior-trees","title":"Why Behavior Trees?","text":"<p>Maintainable - Easy to modify</p> <p>Scalable - Parts of sub-trees are modular and can be encapsulated</p> <p>Reusable - Sub-trees can be reused in different places</p> <p>Clear visualization and interpretation</p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#types-of-nodes","title":"Types of Nodes","text":"<ul> <li>Execution Nodes<ul> <li>Condition Nodes</li> <li>Action Nodes</li> </ul> </li> <li>Decorator Nodes<ul> <li>Not Node</li> </ul> </li> <li>Control Flow Nodes<ul> <li>Sequence Nodes</li> <li>Fallback Nodes</li> </ul> </li> </ul>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#execution-nodes-condition-nodes","title":"Execution Nodes - Condition Nodes","text":"<p>Condition nodes have a status of either SUCCESS or FAILURE</p> <p> </p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#execution-nodes-action-nodes","title":"Execution Nodes - Action Nodes","text":"<p>Action nodes can either be active or inactive</p> <p>An inactive node's status is not checked by the behavior tree, it is shown in white</p> <p>below</p> <p>An active node's status is checked, it can either be SUCCESS (green), RUNNING (blue) or FAILURE (red)</p> <p> </p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#decorator-nodes-not-nodes","title":"Decorator Nodes - Not Nodes","text":"<p>The not node must have one condition node has a child and inverts the status of the child.</p> <p>If the child's status is SUCCESS, the not node's status will be FAILURE.</p> <p>If the child's status is FAILURE, the not node's status will be SUCCESS.</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#control-flow-nodes-fallback-nodes","title":"Control Flow Nodes - Fallback Nodes","text":"<p>These nodes are shown with a ?</p> <p>This node returns FAILURE if and only if all of its children return FAILURE</p> <p>If one of its children return RUNNING or SUCCESS, it returns RUNNING or SUCCESS and no subsequent children's statuses are check</p> <p>Below shows a typical example, where an action will only be performed if all of the preceding conditions are false. In this case a drone will only be armed if it is not already armed, it is in o\ufb00board mode and it is stationary</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#control-flow-nodes-sequence-nodes","title":"Control Flow Nodes - Sequence Nodes","text":"<p>These nodes are shown with a \"-&gt;\"</p> <p>This node returns SUCCESS if and only if all of its children return SUCCESS</p> <p>If one of its children return RUNNING or FAILURE, it returns RUNNING or FAILURE and no subsequent children's statuses are check</p> <p>Below shows a typical example where preceding conditions must be true in order for an action to be performed. In this case the drone will land if the IMU times out and it is in o\ufb00board mode</p> <p></p>"},{"location":"docs/robot/configuration/","title":"Configuration","text":""},{"location":"docs/robot/logging/","title":"Logging","text":""},{"location":"docs/robot/static_transforms/","title":"Index","text":""},{"location":"docs/robot/static_transforms/#frame-conventions","title":"Frame Conventions","text":"<p>Each robot has its own map frame that represents the starting position of the robot. The map frame is expected to be in ENU (East-North-Up) convention. </p> <p>The robot is in the base_link frame.</p>"},{"location":"docs/simulation/","title":"Simulation","text":"<p>We primarily support Isaac Sim. In the future we plan to support Gazebo.</p>"},{"location":"docs/simulation/docker_network/","title":"Docker network","text":""},{"location":"docs/simulation/docker_network/#overview","title":"Overview","text":"<p>The details of the docker containers setup is in the <code>docker-compose.yaml</code> file in the <code>AirStack</code> directory.</p> <p>Isaac Sim, the ground control station, and robots all get created on the same default Docker bridge network. This lets them communicate with ROS2 on the same network.</p> <p>Each robot has its own ROS_DOMAIN_ID.</p>"},{"location":"docs/simulation/docker_network/#ssh-into-robots","title":"SSH into Robots","text":"<p>The <code>ground-control-station</code> and <code>docker-robot</code> containers are setup with ssh daemon, so you can ssh into the containers using the IP address.</p> <p>You can get the IP address of each container by running the following command:</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [CONTAINER-NAME]\n</code></pre> <p>Then ssh in, for example:</p> <pre><code>ssh root@172.18.0.6\n</code></pre> <p>The ssh password is <code>airstack</code>.</p>"},{"location":"docs/simulation/isaac_sim/","title":"Isaac Sim","text":"<p>The primary simulator we support is NVIDIA Isaac Sim.  We chose Isaac Sim as the best balance between photorealism and physics simulation.</p>"},{"location":"docs/simulation/isaac_sim/#usd-file-naming-conventions","title":"USD File Naming Conventions","text":"<p>AirStack uses the following file naming conventions:</p> <p>Purely 3D graphics</p> <ul> <li> <p><code>*.prop.usd</code> \u27f5 simply a 3D model with materials, typically encompassing just a single object. Used for individual assets or objects (as mentioned earlier), representing reusable props.</p> </li> <li> <p><code>*.stage.usd</code> \u27f5 an environment composed of many props, but with no physics, no simulation, no robots. simply scene graphics</p> </li> </ul> <p>Simulation-ready</p> <ul> <li> <p><code>*.robot.usd</code> \u27f5 a prop representing a robot plus ROS2 topic and TF publishers, physics, etc.</p> </li> <li> <p><code>*.scene.usd</code> \u27f5 an environment PLUS physics, simulation, or robots</p> </li> </ul>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/","title":"AirLab AirStack Extension","text":"<p>The AirStack extension for IsaacSim does two main things. It creates an Ascent Omnigraph Node which runs the Ascent SITL and updates the position of a drone model in IsaacSim based on the SITL. It also creates a panel for listing, attaching to, and killing tmux sessions.</p>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/#ascent-omnigraph-node","title":"Ascent OmniGraph Node","text":"<p>The Ascent OmniGraph node takes as input a domain id, node namespace and drone prim. It runs the Ascent SITL, mavproxy, and mavros and takes care of keeping the SITL time synced with IsaacSim's time. Mavros is run using the inputted domain id and node namespace. The drone prim's position is set based off of the position of the drone in the SITL. The drone prim doesn't do collision and will pass through objects in the IsaacSim world.</p> <p>The way the SITL is synced with IsaacSim is by running the SITL in gdb with a breakpoint on the functin that advances the SITL time. Every time this function is called, our code is run by injecting a library using the LD_PRELOAD trick. Our code runs a client socket that talks to a server socket running in the AirStack IsaacSim extension which tells it how long to sleep based off the current SITL and IsaacSim time.</p> <p>The Ascent OmniGraph node is shown below:</p> <p></p>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/#tmux-panel","title":"TMUX Panel","text":"<p>This is a panel for listing, attaching to, and killing any running TMUX sessions. The Ascent SITL, mavproxy, and mavros are run in a TMUX sesion, so this is mainly for debugging those and probably doesn't need to be interacted with by most users. A list of TMUX sessions is displayed in the panel. It doesn't auto refresh so you have to manually click the refresh button to display any changes in the list of sessions. For each session, there is an <code>Attach</code> button and a <code>Kill</code> button. The <code>Attach</code> button will bring up an <code>xterm</code> window with the TMUX session. The <code>Kill</code> button will kill the TMUX session.</p> <p>The TMUX panel is shown below:</p> <p></p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/","title":"Export Unreal Engine to Isaac Sim","text":"<p>A robot needs a scene to interact in. A scene can be created in any 3D modeling program, though we have found it easiest to export stages from Unreal Engine. This document explains how to export an Unreal Engine environment to an Isaac Sim stage, then how to convert the stage to a physics-enabled scene. </p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#exporting-unreal-engine-environments-to-isaac-sim-stages","title":"Exporting Unreal Engine Environments to Isaac Sim Stages","text":"<p>Generally, Unreal Engine environments can be found on Epic Games' Fab Marketplace. For example, the Open World Demo Collection is a free collection of outdoor environments.</p> <p>The below video explains how to export an Unreal Engine environment to an Isaac Sim stage.</p> <p>You can save this file as <code>[YOUR_ENVIRONMENT_NAME].stage.usd</code>.</p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#export-tips","title":"Export Tips","text":"<p>Complexity: Generally, it works best to export levels that are designed for UE4.27 and below. The advanced rendering features from UE5, i.e. nanite and lumen, aren't compatible with Omniverse.  If the level is optimized for the older UE4, then it's more compatible.</p> <p>Omniverse doesn't perform well with large amounts of vegetation. Anything with complex vegetation takes a long long time to load. Procedural foliage doesn't export well either. Simple geometries like buildings and rocks work better. </p> <p>That said you can still achieve photorealism by substituting complex geometries for high quality textures. Isaac seems to do fine with high quality textures.</p> <p>Optimization: After exporting, edit the file with USD Composer and run the Scene Optimizer extension for faster performance. USD Composer can be installed via Omniverse Launcher.</p> <p>Verify the Scale:  The Omniverse exporter exports in centimeters, but Isaac Sim natively works in meters. For consistency, follow these steps to change the scene units to be meters.</p> <p>To check the scale of the scene, you can add a cube in Isaac Sim and compare it to the exported scene. The cube is 1m x 1m x 1m.</p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#turn-a-stage-into-a-physics-enabled-scene","title":"Turn a Stage into a Physics-Enabled Scene","text":"<p>Adding physics to the stage is as simple as adding a <code>Physics</code> property with the \"Colliders Preset\", as described in the Isaac docs. Then save the scene as <code>[YOUR_ENVIRONMENT_NAME].scene.usd</code> to clarify that it's a physics-enabled scene.</p> <p>You're now ready to add robots to the scene on the next page.</p>"},{"location":"docs/simulation/isaac_sim/scene_setup/","title":"AirStack Scene Setup","text":""},{"location":"docs/simulation/isaac_sim/scene_setup/#creating-a-new-scene-with-robots","title":"Creating a New Scene with Robots","text":"<p>The easiest way to create a scene is to copy and customize an existing scene.</p> <p>Example scenes are located on the AirLab Nucleus Server under Projects &gt; AirStack. This can be opened in Isaac's Content Browser: </p> <p>For example, <code>simple_tree_one_drone.scene.usd</code> looks like this: </p> <p>The example scenes are setup with the following:</p> <ul> <li>A \"World\" prim, which is the root of the scene</li> <li>The Root layer is set to use meters as the unit of length</li> <li>Prims that make up the scene. Scene prims should have collision physics enabled with Colliders Preset (Property &gt; Add &gt; Physics &gt; Collider Preset)</li> <li>Robot instances, added to the scene as a reference to the robot USD file. Currently this file is Library &gt; Assets &gt; Ascent_Aerosystems &gt; TEMPLATE_spirit_uav.robot.usd</li> <li>The robot has default sensors added, including a LiDAR and stereo cameras</li> <li>Sensors publish to ROS using the attached ActionGraph</li> <li>Robot dynamics are controlled by the AirStack Extension</li> </ul>"},{"location":"docs/simulation/isaac_sim/scene_setup/#configure-robot-name-and-ros_domain_id","title":"Configure Robot Name and ROS_DOMAIN_ID","text":"<p>Under the Spirit drone prim is an <code>ActionGraph</code> component, which is an Omnigraph. This component is used to configure the ROS publishers for the robot. The <code>ActionGraph</code> component has the following fields to configure:</p> <ul> <li><code>ROBOT_NAME</code>: The name of the robot. This is used as the top-level namespace for ROS topics.</li> <li><code>ROS_DOMAIN_ID</code>: The ROS domain ID. This sets the <code>ROS_DOMAIN_ID</code> environment variable for DDS networking.</li> </ul> <p>The Omnigraph has subgraphs for each ROS publisher type. For example, TFs, Images, and PointClouds. The top-level <code>robot_name</code> and <code>domain_id</code> fields get fed into the subgraphs. </p> <p>To create a new robot, duplicate the drone prim instance and adjust the <code>ROBOT_NAME</code> and <code>ROS_DOMAIN_ID</code> fields to be unique.</p>"},{"location":"docs/simulation/isaac_sim/scene_setup/#customizing-the-omnigraph","title":"Customizing the Omnigraph","text":"<p>Common pre-built graphs for ROS may be added through the top menu bar: <code>Isaac Utils &gt; Common OmniGraphs</code>. This is helpful for creating various sensor publishers.</p> <p>We recommend organizing your work into sub-graphs. Copy your omnigraph template them into the top-level <code>Omnigraph</code> component, named \"ActionGraph\". Connect the <code>robot_name</code> and <code>domain_id</code> fields to your workflow. Then, select all the nodes in your workflow, right-click, and create a subgraph.</p>"},{"location":"docs/simulation/isaac_sim/scene_setup/#frame-conventions","title":"Frame Conventions","text":"<p>Isaac Sim uses Forward-Left-Up (FLU) coordinate frame conventions.  However, MAVROS and AirStack use East-North-Up (ENU). </p> <p>To address this, the origin of the robot lives under a prim called <code>map_FLU</code>. Then AirStack publishes a static transform (<code>static_transforms.launch.xml</code>) from <code>map_FLU</code> to <code>map</code>, which is in ENU. The transform is a 90 degree rotation about the Z-axis. </p> <p>The resulting TF tree looks like this: </p>"},{"location":"git-hooks/","title":"Git Hooks","text":"<p>This directory contains git hooks used in the AirStack repository.</p>"},{"location":"git-hooks/#available-hooks","title":"Available Hooks","text":""},{"location":"git-hooks/#docker-versioning-hook","title":"Docker Versioning Hook","text":"<p>The <code>update-docker-image-tag.pre-commit</code> hook automatically updates the <code>DOCKER_IMAGE_TAG</code> in the <code>.env</code> file with the current git commit hash whenever Docker-related files (Dockerfile or docker-compose.yaml) are modified. It also adds a comment above the variable indicating that the value is auto-generated from the git commit hash.</p> <p>This ensures that Docker images are always tagged with the exact commit they were built from, eliminating version conflicts between parallel branches.</p>"},{"location":"git-hooks/#installation","title":"Installation","text":"<p>To install the hooks:</p> <ol> <li> <p>Copy the hook to your local .git/hooks directory:    <pre><code>cp git-hooks/docker-versioning/update-docker-image-tag.pre-commit .git/hooks/pre-commit\n</code></pre></p> </li> <li> <p>Make sure the hook file is executable:    <pre><code>chmod +x .git/hooks/pre-commit\n</code></pre></p> </li> </ol>"},{"location":"git-hooks/#how-the-docker-versioning-hook-works","title":"How the Docker Versioning Hook Works","text":"<ol> <li>When you commit changes, the hook checks if any Dockerfile or docker-compose.yaml files are being committed</li> <li>If Docker-related files are detected, it updates the DOCKER_IMAGE_TAG in the .env file with the current git commit hash and adds a comment above the variable</li> <li>The modified .env file is automatically added to the commit</li> </ol> <p>This approach eliminates version conflicts between parallel branches by ensuring Docker images are tagged with the exact commit they were built from.</p>"},{"location":"git-hooks/docker-versioning/","title":"Docker Versioning Git Hook","text":"<p>This directory contains a git hook that automatically updates the Docker image tag with the current git commit hash.</p>"},{"location":"git-hooks/docker-versioning/#hook-update-docker-image-tagpre-commit","title":"Hook: update-docker-image-tag.pre-commit","text":"<p>This pre-commit hook automatically updates the <code>DOCKER_IMAGE_TAG</code> in the <code>.env</code> file with the current git commit hash whenever Docker-related files (Dockerfile or docker-compose.yaml) are modified.</p>"},{"location":"git-hooks/docker-versioning/#features","title":"Features","text":"<ul> <li>Automatically updates <code>DOCKER_IMAGE_TAG</code> with the git commit hash</li> <li>Adds a comment above the variable indicating it's auto-generated</li> <li>Only triggers when Docker-related files are modified</li> <li>Automatically stages the modified .env file for commit</li> </ul>"},{"location":"git-hooks/docker-versioning/#installation","title":"Installation","text":"<p>To install the hook:</p> <ol> <li> <p>Copy the hook to your local .git/hooks directory:    <pre><code>cp update-docker-image-tag.pre-commit ../../.git/hooks/pre-commit\n</code></pre></p> </li> <li> <p>Make sure the hook file is executable:    <pre><code>chmod +x ../../.git/hooks/pre-commit\n</code></pre></p> </li> </ol>"},{"location":"git-hooks/docker-versioning/#benefits","title":"Benefits","text":"<ul> <li>Eliminates version conflicts between parallel branches</li> <li>Ensures Docker images are tagged with the exact commit they were built from</li> <li>Simplifies tracking which version of the code is running in Docker containers</li> <li>Provides a consistent and automated versioning system for Docker images</li> </ul>"},{"location":"ground_control_station/installation/","title":"Index","text":"<p>Scripts to install on machine, todo. Maybe something with ansible? Bash scripts could work too.</p>"},{"location":"ground_control_station/ros_ws/","title":"Index","text":"<p>Eventually we will have some ground control station that enables commanding all robots simultaneously.</p> <p>Envision a GUI like an RTS game, where you have a global 3D map you can move around, and can also see and control your \"units\" on the map. A minimap will help with understanding where you are. Envision the map being built up over time in 3D as your agents explore.</p>"},{"location":"ground_control_station/ros_ws/src/mission_manager/","title":"Mission Manager","text":"<p>This package handles the allocation of tasks for the multiple agents. It assigned agents to search or track. For search it divides the search space.</p> <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre> <p>Debugging this node <pre><code>ros2 run --prefix 'gdb -ex run --args'  mission_manager mission_manager_node\n</code></pre></p> <pre><code>ros2 launch mission_manager mission_manager_launch.py\nros2 run rviz2 rviz2\n</code></pre>"},{"location":"ground_control_station/ros_ws/src/ros2tak_tools/","title":"TAK Tools ROS 2 Package","text":"<p><code>ros2tak_tools</code> is a ROS 2 package designed for integrating TAK (Tactical Assault Kit) functionalities within ROS 2. It includes tools for publishing and subscribing to Cursor-On-Target (CoT) events, interfacing with TAK servers, and setting up search and rescue missions.</p>"},{"location":"ground_control_station/ros_ws/src/ros2tak_tools/#features","title":"Features","text":"<ul> <li>ROS 2 to TAK Communication: Send ROS 2 messages to TAK using CoT events.</li> <li>CoT to ROS 2 Communication: Receive CoT events from TAK and publish as ROS 2 messages.</li> <li>Mission Planning: Custom tools to create and manage search missions using CoT and ROS.</li> </ul>"},{"location":"ground_control_station/ros_ws/src/rqt_ground_control_station/","title":"RQT Python GroundControlStation","text":"<p>If you <code>colcon build</code> this package in a workspace and then run \"rqt --force-discover\" after sourcing the workspace, the plugin should show up as \"Ground Control Station\" in \"Miscellaneous Tools\" in the \"Plugins\" menu.</p> <p>You can use the <code>generate_rqt_py_package.sh</code> script to generate a new package by doing the following from the rqt_ground_control_station directory</p> <pre><code>./generate_rqt_py_package.sh [package name] [class name] [plugin title]\n</code></pre> <p>[package name] will be the name of the package and a directory with this name will be created above <code>rqt_ground_control_station/</code>. [class name] is the name of the class in <code>src/[package name]/template.py</code>. [plugin title] is what the plugin will be called in the \"Miscellaneous Tools\" menu.</p> <p>For example,</p> <pre><code>cd rqt_ground_control_station/\n./generate_rqt_py_package.sh new_rqt_package ClassName \"Plugin Title\"\n</code></pre>"},{"location":"ground_control_station/ros_ws/src/rqt_py_template/","title":"RQT Python Template","text":"<p>If you <code>colcon build</code> this package in a workspace and then run \"rqt --force-discover\" after sourcing the workspace, the plugin should show up as \"PluginTitle\" in \"Miscellaneous Tools\" in the \"Plugins\" menu.</p> <p>You can use the <code>generate_rqt_py_package.sh</code> script to generate a new package by doing the following from the rqt_py_template directory</p> <pre><code>./generate_rqt_py_package.sh [package name] [class name] [plugin title]\n</code></pre> <p>[package name] will be the name of the package and a directory with this name will be created above <code>rqt_py_template/</code>. [class name] is the name of the class in <code>src/[package name]/template.py</code>. [plugin title] is what the plugin will be called in the \"Miscellaneous Tools\" menu.</p> <p>For example,</p> <pre><code>cd rqt_py_template/\n./generate_rqt_py_package.sh new_rqt_package ClassName \"Plugin Title\"\n</code></pre>"},{"location":"robot/installation/","title":"Index","text":"<p>Scripts to install on machine, todo. Maybe something with ansible? Bash scripts could work too.</p>"},{"location":"robot/ros_ws/src/autonomy/0_interface/drone_safety_monitor/","title":"Index","text":""},{"location":"robot/ros_ws/src/autonomy/0_interface/drone_safety_monitor/#takeofflandingplanner","title":"TakeoffLandingPlanner","text":"<p>Author:</p>"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/","title":"Camera Parameter Server","text":""},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#summary","title":"Summary","text":"<p>The camera parameter server was designed to eliminate the need for multiple nodes to subscribe to each camera individually, reducing unnecessary subscribers and callbacks. Its sole purpose is to listen to the camera info topics, store relevant information about the cameras, and provide it on demand for other nodes to query.</p>"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#configuration","title":"Configuration","text":"<p>The camera parameter server is currently configurable through a non-ROS configuration file. This file allows users to define a list of cameras, specifying their types and topic names. At the top level of the configuration, a base link name is provided to indicate the <code>tf</code> name for the robot's center. Additionally, a parameter called <code>camera_list</code> contains a list of dictionaries, with each dictionary representing an individual camera. Currently, two camera types are supported: monocular and stereo.</p>"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#parameters","title":"Parameters","text":"<p>Below are the parameters needed for the meta level camera parameter server configuration, as well as the camera fields needed to specify individual camera types.</p>"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#meta-level-parameters","title":"Meta Level Parameters","text":"Parameter Description <code>base_link_frame_id</code> The frame name of the base link, or center frame of the robot <code>camera_list</code> A list of dictionaries that define each camera of the system"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#monocular-camera-parameters","title":"Monocular Camera Parameters","text":"Parameter Description <code>camera_name</code> The name of the camera <code>camera_type</code> The type of camera, for monocular being <code>mono</code> <code>camera_info_sub_topic</code> The info topic name for the camera, normally <code>camera_info</code> <code>camera_frame_id</code> The frame name of the camera to find its tf"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#stereo-camera-parameters","title":"Stereo Camera Parameters","text":"Parameter Description <code>camera_name</code> The name of the camera <code>camera_type</code> The type of camera, for stereo being <code>stereo</code> <code>camera_info_sub_topic</code> The info topic name for the camera, normally <code>camera_info</code> <code>left_camera_frame_id</code> The frame name of the left camera for find its tf <code>right_camera_frame_id</code> The frame name of the right camera to find its tf"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#services","title":"Services","text":"Parameter Type Description <code>~/get_camera_params</code> sensor_interfaces/GetCameraParams The service to get info about the desired camera. This provides camera intrinsics, transform frame ids, and baseline if the camera type is a stereo"},{"location":"robot/ros_ws/src/autonomy/1_sensors/camera_param_server/#subscriptions","title":"Subscriptions","text":"Parameter Type Description <code>tf/</code> tfMessage Listens for the tf for the specified cameras <code>~/camera_info</code> sensor_msgs/CameraInfo Listens for the info for specified cameras"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/","title":"MAC-VO","text":""},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#summary","title":"Summary","text":"<p>Currently, the state estimation of our robot relies on MAC-VO, a learning-based stereo visual odometry algorithm. This is purely camera based, and does not rely on additional sensors. On initialization, the node will load the model weights, and then allocate the required memory to store the model on first inference. This process may take some time. Once this process is complete, the inference time should be able to run at around 3 Hz. Documentation on the MAC-VO model can be found here</p> <p>This node is also setup to retrieve camera info on node initialization. This includes camera intrinsics and baseline of the desired camera, and is setup using a service call to the camera parameter server.</p> <p>The output from this node should give the pose estimates of the model, the feature points in 3D space used to estimate the pose, and a visualization image of the points projected onto the RGB image. The pose is currently given in the perspective of the left camera frame.</p>"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#configuration","title":"Configuration","text":"<p>The wrapper that is currently used for interfacing with the non-ROS MAC-VO logic is modified from the one provided here. For our purposes, we wanted modularity in interfacing with the node, so we now have two configuration files:</p> <ul> <li><code>interface_config.yaml</code>: This file specifies the desired camera name, the subscriber and publisher topics, and the size of the image when being fed through inference. This was designed specifically for Airstack</li> <li><code>model_config.yaml</code>: This file is sourced from the official MAC-VO ROS wrapper and defines the structure for creating the MAC-VO model. It also specifies the location of the model weights, currently stored at /root/model_weights/MACVO_FrontendCov.pth within the Docker container.</li> </ul>"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#parameters","title":"Parameters","text":"<p>Below are the parameters for the <code>interface_config.yaml</code>. To find out more about the parameters for the <code>model_config.yaml</code>, please consult the MAC-VO Documentation.</p> Parameter Description <code>camera_name</code> The name of the camera that the visual odometry should process from <code>camera_param_server_client_topic</code> Topic name for the camera parameter server <code>imageL_sub_topic</code> Topic name for the left stereo image, appended with the camera name <code>imageR_sub_topic</code> Topic name for the right stereo image, appended with the camera name <code>pose_pub_topic</code> Topic name for the pose estimate output from the MAC-VO model <code>point_pub_topic</code> Topic name for the point cloud of feature points with covariances used to estimate pose <code>img_pub_topic</code> Topic name for the visualization of the feature points over the rgb for debugging <code>inference_dim_u</code> The width of the images fed into the MAC-VO model, which affects inference rate <code>inference_dim_v</code> The height of the images fed into the MAC-VO model, which affects inference rate"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#services","title":"Services","text":"Parameter Type Description <code>~/get_camera_params</code> sensor_interfaces/GetCameraParams A service to get info about the desired camera"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#subscriptions","title":"Subscriptions","text":"Parameter Type Description <code>~/left/image_rect</code> sensor_msgs/Image The left RGB image from the stereo camera <code>~right/image_rect</code> sensor_msgs/Image The right RGB image from the stereo camera"},{"location":"robot/ros_ws/src/autonomy/2_perception/macvo/#publications","title":"Publications","text":"Parameter Type Description <code>~/visual_odometry_pose</code> nav_msgs/Path Outputs the pose estimate output from the MAC-VO model. <code>~/visual_odometry_points</code> nav_msgs/Path Outputs the point cloud of feature points with covariances used to estimate pose. <code>~/visual_odometry_img</code> nav_msgs/Path Outputs the visualization of the feature points over the rgb for debugging."},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/","title":"README","text":"<p>This package generates a world representation using disparity images. This enables planning in image space by applying C-space expansion in 2.5D disparity images.</p> <p>It generates a pair expansion images: one for foreground expansion and one for background expansion.</p> <p>Currently this package also has the following nodes</p> <ul> <li> <p><code>disparity_expansion</code>: Generate expanded disparity</p> </li> <li> <p><code>disparity_pcd</code>: Disparity image to point cloud.</p> </li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/#who-do-i-talk-to","title":"Who do I talk to?","text":"<ul> <li>geetesh dubey (geeteshdubey@gmail.com)</li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/#license","title":"License","text":"<p>BSD, see LICENSE</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_graph/","title":"Disparity Graph","text":"<p>Contact: Andrew Jong</p> <p>Docs TODO. Help appreciated.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_graph_cost_map/","title":"Disparity Graph Cost Map","text":"<p>Contact: Andrew Jong</p> <p>Docs TODO. Help appreciated.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/b_planners/droan_local_planner/","title":"DROAN Local Planner","text":"<p>This is part of a set of packages that performs local obstacle avoidance from stereo depth. The method is based on the publication \"DROAN - Disparity-space representation for obstacle avoidance.\". Essentially, the local world model makes a C-space expansion around detected obstacles in the disparity image. The local planner then plans a path by scoring the best trajectory from a trajectory library in this expanded space.</p> <p>The packages are</p> <ol> <li>disparity_expansion   (takes in stereo disparity and expands it to a 3D point cloud)</li> <li>disparity_graph        (creates a graph where each node is a drone pose and 3D point cloud observation)</li> <li>disparity_graph_cost_map  (creates a cost map from the disparity graph)</li> <li>droan_local_planner  (this package. uses the cost map to plan a path)</li> </ol> <p>DROAN local planner takes the global plan, and finds the global plan's closest point to the drone. DROAN trims the global plan to be from that point to the end of the global plan.</p> <p>Consequently, DROAN currently does NOT use a waypoint manager, and is NOT guaranteed to reach every waypoint on the global plan, especially if the global plan loops back on itself. Ideally in the future we will use a proper waypoint manager.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/b_planners/takeoff_landing_planner/","title":"Takeoff Landing Planner","text":"<p>Author: John Keller</p> <p>DOCS TODO. Help appreciated.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/b_planners/trajectory_library/","title":"Trajectory Library","text":"<p>Contact: John Keller</p> <p>Defines some basic trajectory classes and functions for generating and manipulating trajectories.</p> <p>Docs TODO. Help appreciated.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/mav_comm/","title":"mav_comm","text":"<p>This repository contains message and service definitions used for mavs. All future message definitions go in here, existing ones in other stacks should be moved here where possible.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/","title":"px4_msgs","text":"<p>ROS 2 message definitions for the PX4 Autopilot project.</p> <p>Building this package generates all the required interfaces to interface ROS 2 nodes with the PX4 internals.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#supported-versions-and-compatibility","title":"Supported versions and compatibility","text":"<p>Depending on the PX4 and ROS versions you want to use, you need to checkout the appropriate branch of this package:</p> PX4 ROS 2 Ubuntu branch v1.13 Foxy Ubuntu 20.04 release/1.13 v1.14 Foxy Ubuntu 20.04 release/1.14 v1.14 Humble Ubuntu 22.04 release/1.14 v1.14 Rolling Ubuntu 22.04 release/1.14 main Foxy Ubuntu 22.04 main main Humble Ubuntu 22.04 main main Rolling Ubuntu 22.04 main"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#messages-sync-from-px4","title":"Messages Sync from PX4","text":"<p>When PX4 message definitions in the <code>main</code> branch of PX4 Autopilot change, a CI/CD pipeline automatically copies and pushes updated ROS message definitions to this repository. This ensures that this repository <code>main</code> branch and the PX4-Autopilot <code>main</code> branch are always up to date. However, if you are using a custom PX4 version and you modified existing messages or created new one, then you have to manually synchronize them in this repository:</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#manual-message-sync","title":"Manual Message Sync","text":"<ul> <li>Checkout the correct branch associated to the PX4 version from which you detached you custom version.</li> <li>Delete all <code>*.msg</code> files in <code>msg/</code> and copy all <code>*.msg</code> files from <code>PX4-Autopilot/msg/</code> in it. Assuming that this repository and the PX4-Autopilot repository are placed in your home folder, you can run:   <pre><code>rm -f ~/px4_msgs/msg/*.msg\ncp ~/PX4-Autopilot/msg/*.msg ~/px4_msgs/msg/\n</code></pre></li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#install-build-and-usage","title":"Install, build and usage","text":"<p>Check Using colcon to build packages to understand how this can be built inside a workspace. Check the PX4 ROS 2 User Guide section on the PX4 documentation for further details on how this integrates PX4 and how to exchange messages with the autopilot.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#bug-tracking-and-feature-requests","title":"Bug tracking and feature requests","text":"<p>Use the Issues section to create a new issue. Report your issue or feature request here.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#questions-and-troubleshooting","title":"Questions and troubleshooting","text":"<p>Reach the PX4 development team on the PX4 Discord Server.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/CONTRIBUTING/","title":"Contributing","text":"<p>Do not commit changes directly to this repository that change the message definitions. All the message definitions are directly generated from the uORB msg definitions on the PX4 Firmware repository. Any fixes or improvements one finds suitable to apply to the message definitions should be directly done on the uORB message files. The deployment of these are taken care by a Jenkins CI/CD stage.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/CONTRIBUTING/#contributing-to-the-px4-firmware-repository-or-to-this-repository-not-including-message-definitions","title":"Contributing to the PX4 Firmware repository (or to this repository, not including message definitions)","text":"<p>Follow the <code>Contributing</code> guide from the PX4 Firmware repo.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/","title":"Trajectory Controller","text":""},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#overview","title":"Overview","text":"<p>The trajectory controller takes in a trajectory and publishes a tracking point, which a controller should make the drone fly to, and a look ahead point, which a planner should plan from. The trajectory controller can interpret a trajectory as a standalone complete track, like a figure eight or racetrack pattern for tuning controls, or as separate segments that should be stitched together, for example trajectories output by a local planner.</p> <p>The trajectory controller tries to keep the tracking point ahead of the robot in a pure pursuit fashion. The robot's position, the red X in the figure below, is projected onto the trajectory. A sphere, shown in 2d as the cyan circle, is placed around this projected point and the intersection between the sphere and the forward point on the trajectory is used as the tracking point, the blue X. The lookahead point, the orange X, is a fixed time duration along the trajectory.</p> <p></p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#parameters","title":"Parameters","text":"Parameter Description <code>tf_prefix</code> The tf names published are prefixed with the string in this parameter. Tfs are published at the tracking point and lookahead point. There are stabilized versions, the same but with zero pitch and roll, of these two tfs that are also published. <code>target_frame</code> The tracking point and lookahead point are published in this frame. <code>look_ahead_time</code> How far ahead of the tracking point the lookahead point will be in seconds. <code>min_virtual_tracking_velocity</code> If the velocity on the trajectory is less than this, the tracking point will just move forward in time instead of using the sphere to keep the tracking point ahead of the drone. The units are m/s. <code>sphere_radius</code> This is the radius of the sphere used to determine the position of the tracking point. Making it larger pushes the tracking point farther ahead. <code>search_ahead_factor</code> To search for the point on the trajectory that intersects with the sphere, the algorithm checks a certain distance ahead along the trajectory. This distance is given by <code>sphere_radius * search_ahead_factor</code>. If the trajectory zigzags a lot relative to the size of the sphere, it's possible that the algorithm wouldn't iterate far enough along the trajectory to find the point where it intersects with the sphere. If a large value for this parameter is used and the trajectory loops back on itself, it is possible that this would cause the tracking point to jump ahead and skip a portion of the trajectory. In almost all cases, this parameter shouldn't need to be changed. <code>traj_vis_thickness</code> The thickness of the trajectory visualization markers. ## Services"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#trajectory-modes","title":"Trajectory Modes","text":"<p>There are several modes that the trajectory controller can be placed in with a service call to the <code>set_trajectory_mode</code> service: See TrajectoryMode.srv.</p> Mode Description TRACK This interprets a trajectory subscribed on <code>~/trajectory_override</code> as a complete trajectory that the controller will follow. It is usually used for taking off, landing, and tuning controls on fixed trajectories like figure eights, racetracks, circles, etc... ADD_SEGMENT This interprets a trajectory subscribed on <code>~/trajectory_segment_to_add</code> as a segment of a trajectory which will get stitched onto the current trajectory at the closest point to the start of the new segment. This is usually published by a local planner. Ideally it is published at the location of the lookahead point, which is a fixed time ahead of the tracking point. This fixed time should be greater than the time it takes to plan. For example, if the lookahead point is one second ahead of the tracking point, the local planner should be always take less than one second to plan otherwise the tracking point would already be past the start of the plan. If this happens, the trajectory will fail to be stitched and will be ignored. PAUSE This causes the tracking point to stop where it is. REWIND This makes the tracking point go backwards along the trajectory. This mode is usually used to make the drone blindly backtrack along its trajectory to get it out of a situation it is stuck in. ROBOT_POSE This makes the tracking point and lookahead point always be at the same position as the drone's odometry. This is useful for before takeoff, when the robot may be carried around so that the location where the takeoff starts is at the drone's position."},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#subscriptions","title":"Subscriptions","text":"Topic Type Description <code>~/odometry</code> nav_msgs/Odometry Odometry of the robot. <code>~/trajectory_segment_to_add</code> airstack_msgs/TrajectoryXYZVYaw For ADD_SEGMENT mode, this is the trajectory segment to add to the current trajectory. <code>~/trajectory_override</code> airstack_msgs/TrajectoryXYZVYaw For TRACK mode, this overrides the current trajectory and makes the robot follow this directly."},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#publications","title":"Publications","text":"Topic Type Description <code>~/tracking_point</code> TODO TODO <code>~/look_ahead</code> TODO TODO <code>~/traj_drone_point</code> TODO TODO <code>~/virtual_tracking_point</code> TODO TODO <code>~/closest_point</code> TODO TODO <code>~/trajectory_completion_percentage</code> TODO TODO <code>~/trajectory_time</code> TODO TODO <code>~/tracking_error</code> TODO TODO <code>~/velocity_pub</code> TODO TODO <code>~/debug_markers</code> TODO TODO <code>~/trajectory_vis</code> TODO TODO"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/","title":"Random Walk Global Planner Baseline","text":"<p>The Random Walk Planner serves as a baseline global planner for stress testing system autonomy. Unlike more informed and intelligent planners, the Random Walk Planner generates a series of random trajectories to evaluate system robustness. Using the map published by VDB, the planner will generate and publish multiple linked straight-line trajectories, checking for collisions along these paths.</p> <p> The blue line is the global plan generated by the random walk. The yellow line shows the past trajectory followed by the local planner when pursuing the previous random global plans.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#functionality","title":"Functionality","text":"<p>Upon activation by the behavior tree, the Random Walk Planner will:</p> <ol> <li>Generate a specified number of straight-line path segments.</li> <li>Continuously monitor the robot's progress along the published path.</li> <li>Once the robot completes the current path, a new set of paths will be generated.</li> </ol> <p>This loop continues, allowing the system to explore various trajectories and stress test the overall autonomy stack.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#parameters","title":"Parameters","text":"Parameter Description <code>num_paths_to_generate</code> Number of straight-line paths to concatenate into a complete trajectory. <code>max_start_to_goal_dist_m</code> Maximum distance (in meters) from the start point to the goal point for each straight-line segment. <code>checking_point_cnt</code> Number of points along each straight-line segment to check for collisions. <code>max_z_change_m</code> Maximum allowed change in height (z-axis) between the start and goal points. <code>collision_padding_m</code> Extra padding (in meters) added around a voxel's dimensions when checking for collisions. <code>path_end_threshold_m</code> Distance threshold (in meters) for considering the current path completed and generating a new one. <code>max_yaw_change_degrees</code> Maximum allowed change in angle (in radians) between consecutive straight-line segments to ensure a relatively consistent direction. <code>robot_frame_id</code> The frame name for the robot's base frame to look up the transform from the robot position to the world."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#services","title":"Services","text":"Parameter Type Description <code>~/global_plan_toggle</code> std_srvs/Trigger A toggle switch to turn on and off the random walk planner."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#subscriptions","title":"Subscriptions","text":"Parameter Type Description <code>~/sub_map_topic</code> visualization_msgs/Marker Stores the map representation that is output from the world or local map topic; currently using vdb local map. <code>~/tf</code> geometry_msgs/TransformStamped Stores the transform from the robot to the world."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#publications","title":"Publications","text":"Parameter Type Description <code>~/pub_global_plan_topic</code> nav_msgs/Path Outputs the global plan that is generated from the random walk planner."},{"location":"robot/ros_ws/src/autonomy/5_behavior/rqt_fixed_trajectory_generator/","title":"RQT Python FixedTrajectoryGenerator","text":"<p>If you <code>colcon build</code> this package in a workspace and then run \"rqt --force-discover\" after sourcing the workspace, the plugin should show up as \"Fixed Trajectory Generator\" in \"Miscellaneous Tools\" in the \"Plugins\" menu.</p> <p>You can use the <code>generate_rqt_py_package.sh</code> script to generate a new package by doing the following from the rqt_fixed_trajectory_generator directory</p> <pre><code>./generate_rqt_py_package.sh [package name] [class name] [plugin title]\n</code></pre> <p>[package name] will be the name of the package and a directory with this name will be created above <code>rqt_fixed_trajectory_generator/</code>. [class name] is the name of the class in <code>src/[package name]/template.py</code>. [plugin title] is what the plugin will be called in the \"Miscellaneous Tools\" menu.</p> <p>For example,</p> <pre><code>cd rqt_fixed_trajectory_generator/\n./generate_rqt_py_package.sh new_rqt_package ClassName \"Plugin Title\"\n</code></pre>"},{"location":"simulation/gazebo/","title":"Index","text":"<p>In the future we could support Gazebo.</p>"},{"location":"simulation/isaac-sim/sitl_integration/docs/","title":"Usage","text":"<p>To enable this extension, run Isaac Sim with the flags --ext-folder {path_to_ext_folder} --enable {ext_directory_name}</p>"},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/","title":"Changelog","text":""},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/#010-2024-07-10","title":"[0.1.0] - 2024-07-10","text":""},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/#added","title":"Added","text":"<ul> <li>Initial version of TEST_EXTENSION_TITLE Extension</li> </ul>"},{"location":"simulation/isaac-sim/sitl_integration/drag_and_drop/","title":"Index","text":"<p>Download the AscentAeroSystemsSITLPackage.zip from Google Drive and unzip into this directory.</p>"}]}